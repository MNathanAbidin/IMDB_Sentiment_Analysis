{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No6Llo199MVN"
      },
      "source": [
        "1. Instalasi dan Setup Awal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i0YdVQJc9Leu"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mre\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnltk\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vASTzBsi9dBu"
      },
      "source": [
        "2. Fungsi Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opjz9e259hUI",
        "outputId": "441555c4-608e-44bc-c700-29801c70510f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loveee movi\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Membersihkan teks: lowercase, hapus HTML/URL/tanda baca, stopwords, dan stemming\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)          # Hapus HTML tags\n",
        "    text = re.sub(r'http\\S+', '', text)        # Hapus URL\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)    # Hapus non-alphabet\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()   # Hapus spasi berlebih\n",
        "\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stopwords.words('english')]  # Stopwords\n",
        "    words = [PorterStemmer().stem(word) for word in words]                     # Stemming\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Contoh uji fungsi\n",
        "print(preprocess_text(\"<p>I LOVEEEE this movie!!! https://example.com </p>\"))  # Output: \"lovee movi\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VglEgCjs9my9"
      },
      "source": [
        "3. Load & Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JEOtQWP9r9T",
        "outputId": "1acc0f05-0560-44aa-a7fe-f4a9870dcd66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contoh data training: that kept ask mani fight scream match swear gener mayhem permeat minut comparison also stand think o ...\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/dataset/IMDB Dataset.csv\")  # Ganti dengan path Anda\n",
        "df['clean_review'] = df['review'].apply(preprocess_text)  # Apply preprocessing\n",
        "\n",
        "# Encode label\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['sentiment'])  # positive=1, negative=0\n",
        "\n",
        "# Split data\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['clean_review'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Contoh data training:\", train_texts[0][:100], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnO0SKqW9uq7"
      },
      "source": [
        "4. Tokenisasi dengan BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1wDdfiy9yUB",
        "outputId": "37a54462-0864-432d-b959-bee4795d86fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jumlah batch training: 2500\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenisasi data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "# Konversi ke TensorFlow Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).shuffle(1000).batch(16)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        ")).batch(16)\n",
        "\n",
        "print(\"Jumlah batch training:\", len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QgSSHeI92fD"
      },
      "source": [
        "5. Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH06sgCt95d7",
        "outputId": "239fa618-7c5f-4543-f212-88442836370f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            " 121/2500 [>.............................] - ETA: 34:40:46 - loss: 0.6061 - accuracy: 0.6627"
          ]
        }
      ],
      "source": [
        "# Inisialisasi model\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Setup optimizer\n",
        "num_train_steps = len(train_dataset) * 3\n",
        "optimizer, _ = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=num_train_steps)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Training\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=3\n",
        ")\n",
        "\n",
        "# Evaluasi\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"\\nAkurasi test: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UT4i0Bz98DX"
      },
      "source": [
        "6. Simpan Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD-GdBZe9-0l"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Lokasi penyimpanan\n",
        "save_dir = \"/content/drive/MyDrive/bert_imdb_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Simpan model & tokenizer\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Alternatif download langsung\n",
        "!zip -r bert_model.zip {save_dir}\n",
        "from google.colab import files\n",
        "files.download(\"bert_model.zip\")\n",
        "\n",
        "print(f\"Model disimpan di: {save_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul0XADjU-A7R"
      },
      "source": [
        "7. Fungsi Prediksi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YaDNxAD-EiG"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"Prediksi sentimen dari teks input\"\"\"\n",
        "    cleaned_text = preprocess_text(text)  # Preprocess konsisten\n",
        "    inputs = tokenizer(\n",
        "        cleaned_text,\n",
        "        return_tensors=\"tf\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256\n",
        "    )\n",
        "    outputs = model(inputs)\n",
        "    probs = tf.nn.softmax(outputs.logits, axis=-1)\n",
        "    label = tf.argmax(probs, axis=1).numpy()[0]\n",
        "    return \"positive\" if label == 1 else \"negative\"\n",
        "\n",
        "# Contoh penggunaan\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained(save_dir)\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained(save_dir)\n",
        "\n",
        "print(predict_sentiment(\"This movie sucks!\", loaded_model, loaded_tokenizer))  # Output: negative"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
